# -*- coding: utf-8 -*-
"""20211027.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WvrX5PWDmWJs5VtUim_HRS1jgvIQqCu4
"""

import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import numpy as np
from google.colab import files
file_uploaded = files.upload()

# 1. 데이터 불러오기
data = pd.read_csv('/content/ThoraricSurgery.csv')

# 2. 데이터 확인하기
data.head()

data = data.values

# 3. x,y 나누기
x_data = data[:,0:17]
y_data = data[:,17]

dataset_stats = data.describe()
dataset_stats = dataset_stats.transpose()
dataset_stats

## data normalization
def min_max_norm(x):
  return (x - dataset_stats['min']) / (dataset_stats['max'] - dataset_stats['min'])

def standard_norm(x):
  return (x - dataset_stats['mean']) / dataset_stats['std']

normed_x_data =  min_max_norm(x_data)

# 4. 전체 데이터에서 학습데이터와 테스터데이터(0.2) 구분

x_train1, x_test, y_train1, y_test = train_test_split(x_data, y_data, test_size=0.3, shuffle=True)
x_train, x_valid, y_train, y_valid = train_test_split(x_train1, y_train1, test_size=0.2, shuffle=True)

# 5. 딥러닝 설계  ---> kernel_initializer 추가 he/xavier
input_layer = tf.keras.layers.Input(shape=(17,))
x = tf.keras.layers.Dense(50, activation='sigmoid',kernel_initializer=tf.keras.initializers.he_normal())(input_layer)
x = tf.keras.layers.Dense(100, activation='sigmoid',kernel_initializer=tf.keras.initializers.he_normal())(x)
x = tf.keras.layers.Dense(300, activation='sigmoid',kernel_initializer=tf.keras.initializers.he_normal())(x)
out_layer = tf.keras.layers.Dense(1, activation=None)(x)

model = tf.keras.Model(inputs=[input_layer], outputs=[out_layer])
model.summary()

import math
def step_decay(epoch):
	initial_lrate = 0.001
	drop = 0.98
	epochs_drop = 50.0
	lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))
	return lrate

modelpath = "./{epoch:02d}-{val_loss:.4f}.h5"
callback_list = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20),
                 tf.keras.callbacks.ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True),
                 tf.keras.callbacks.LearningRateScheduler(step_decay, verbose=1)]

# 6. model.complie
loss = tf.keras.losses.mean_squared_error
optimizer = tf.keras.optimizers.SGD(lr=0.0004)
metrics = tf.keras.metrics.RootMeanSquaredError()
model.compile(loss = loss,
              optimizer = optimizer,
              metrics = [metrics])

# 7. model.fit
result=model.fit(x_train, y_train, epochs = 200, batch_size = 10, validation_data=(x_valid, y_valid))

print(result.history.keys())
loss = result.history['loss']
val_loss = result.history['val_loss']

# 8. 그래프 결과 확인
### history에서 loss와 val_loss의 key를 가지는 값들만 추출
loss = result.history['loss']
val_loss = result.history['val_loss']
### loss와 val_loss를 그래프화
epochs = range(1, len(loss) + 1)
plt.subplot(211)  ## 2x1 개의 그래프 중에 1번째
plt.plot(epochs, loss, 'b-', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

### history에서 mean_absolute_error val_mean_absolute_error key를 가지는 값들만 추출
rmse = result.history['root_mean_squared_error']
val_rmse = result.history['val_root_mean_squared_error']
epochs = range(1, len(rmse) + 1)
### mean_absolute_error val_mean_absolute_error key를 그래프화
plt.subplot(212)  ## 2x1 개의 그래프 중에 2번째
plt.plot(epochs, rmse, 'b-', label='Training rmse')
plt.plot(epochs, val_rmse, 'r', label='Validation rmse')
plt.title('Training and validation rmse')
plt.xlabel('Epochs')
plt.ylabel('rmse')
plt.legend()

print("\n Test rmse: %.4f" % (model.evaluate(x_test, y_test)[1]))

plt.show()

# 9. test 정확도 확인
print("\n Test Accuracy: %.4f" % (model.evaluate(x_test, y_test)[1]))

"""# NN MNIST"""

import tensorflow as tf
import os

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

print(len(x_train))
print(len(y_train))
print(x_train.shape)
print(y_train.shape)

plt.imshow(x_train[500],cmap=plt.cm.binary)

print(y_train[500])

#index에 0에서 59999사이의 숫자를 지정해보자
index = 387
plt.imshow(x_train[index], cmap=plt.cm.binary)
plt.show()
print((index+1), '번째 이미지의 숫자는 바로', y_train[index],'입니다')

#index에 0에서 59999사이의 숫자를 지정해보자
index = 10000
plt.imshow(x_train[index], cmap=plt.cm.binary)
plt.show()
print((index+1), '번째 이미지의 숫자는 바로', y_train[index],'입니다')

#index에 0에서 59999사이의 숫자를 지정해보자
index = 59999
plt.imshow(x_train[index], cmap=plt.cm.binary)
plt.show()
print((index+1), '번째 이미지의 숫자는 바로', y_train[index],'입니다')

print(x_test.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)

print('최소값', np.min(x_train),'최대값',np.max(x_train))

"""# 정규화"""

x_train_norm = x_train / 255.0
x_test_norm = x_test / 255.0
print('최소값:', np.min(x_train_norm), '최대값:', np.max(x_test_norm))

"""## 행렬 모양 28 *28을 784,1로 변환"""

x_train = x_train_norm.reshape(x_train.shape[0], 784).astype('float32')
x_test = x_test_norm.reshape(x_test.shape[0], 784).astype('float32')

y_train = tf.keras.utils.to_categorical(y_train, 10)
y_test = tf.keras.utils.to_categorical(y_test, 10)

print(y_train[0])

from tensorflow import keras
model = keras.models.Sequential()
model.add(keras.layers.Dense(512, activation='relu', input_shape=(784,)))
model.add(keras.layers.Dense(256, activation='relu'))
model.add(keras.layers.Dense(10, activation='softmax'))

model.summary()

print(len(model.layers))

model.compile(optimizer='adam',
              loss = 'categorical_crossentropy',
              metrics = ['accuracy'])

history = model.fit(x_train, y_train, validation_split=0.1, epochs=10, batch_size=200, verbose=1)

test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=2)
print('test loss : {}'.format(test_loss))
print('test_accuracy: {}'.format(test_accuracy))

# 예측

predicted_result = model.predict(x_test)
predicted_labels = np.argmax(predicted_result,axis=1)

idx = 0
print('model.predict() 결과 :',predicted_result[idx])
print('model이 추론한 가장 가능성이 높은 결과 : ',predicted_labels[idx])
print('실제 데이터의 라벨 :',y_test[idx])

x_test.shape

x_test_result = x_test.reshape(x_test.shape[0], 28, 28)

x_test_result.shape

plt.imshow(x_test_result[idx], cmap=plt.cm.binary)
plt.show()

# 테스트 셋의 오차
y_vloss = history.history['val_loss']

# 학습셋의 오차
y_loss = history.history['loss']

# 그래프로 표현
x_len = np.arange(len(y_loss))
plt.plot(x_len, y_vloss, marker='.', c="red", label='Testset_loss')
plt.plot(x_len, y_loss, marker='.', c="blue", label='Trainset_loss')

# 그래프에 그리드를 주고 레이블을 표시
plt.legend(loc='upper right')
# plt.axis([0, 20, 0, 0.35])
plt.grid()
plt.xlabel('epoch')
plt.ylabel('loss')
plt.show()

"""# 가위바위보 판별 딥러닝"""

## 학습데이터.zip 업로드
from google.colab import files
file_uploaded = files.upload()

file_uploaded = files.upload()

file_uploaded = files.upload()

!unzip "scissor.zip" -d "./scissor"
!unzip "paper.zip" -d "./paper"
!unzip "rock.zip" -d "./rock"

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

from PIL import Image
import os, glob

def resize_images(img_path):
	images=glob.glob(img_path + "/*.jpg")  
    
	print(len(images), " images to be resized.")

    # 파일마다 모두 28x28 사이즈로 바꾸어 저장합니다.
	target_size=(28,28)
	for img in images:
		old_img=Image.open(img)
		new_img=old_img.resize(target_size,Image.ANTIALIAS)
		new_img.save(img, "JPEG")
    
	print(len(images), " images resized.")

# 가위 이미지가 저장된 디렉토리 아래의 모든 jpg 파일을 읽어들여서
image_dir_path = "./scissor/"
resize_images(image_dir_path)

print("가위 이미지 resize 완료!")

# 바위 이미지가 저장된 디렉토리 아래의 모든 jpg 파일을 읽어들여서
image_dir_path = "./rock/"
resize_images(image_dir_path)

print("바위 이미지 resize 완료!")

# 파일마다 모두 28x28 사이즈로 바꾸어 저장합니다.
image_dir_path = "./paper/"
resize_images(image_dir_path)

print("보 이미지 resize 완료!")

import numpy as np

def load_data(img_path, number_of_data=2700):  # 가위바위보 이미지 개수 총합에 주의하세요.
    # 가위 : 0, 바위 : 1, 보 : 2
    img_size=28
    color=3
    #이미지 데이터와 라벨(가위 : 0, 바위 : 1, 보 : 2) 데이터를 담을 행렬(matrix) 영역을 생성합니다.
    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)
    labels=np.zeros(number_of_data,dtype=np.int32)

    idx=0
    for file in glob.iglob(img_path+'/scissor/*.jpg'):
        img = np.array(Image.open(file),dtype=np.int32)
        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사
        labels[idx]=0   # 가위 : 0
        idx=idx+1

    for file in glob.iglob(img_path+'/rock/*.jpg'):
        img = np.array(Image.open(file),dtype=np.int32)
        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사
        labels[idx]=1   # 바위 : 1
        idx=idx+1  
    
    for file in glob.iglob(img_path+'/paper/*.jpg'):
        img = np.array(Image.open(file),dtype=np.int32)
        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사
        labels[idx]=2   # 보 : 2
        idx=idx+1
        
    print("학습데이터(x_train)의 이미지 개수는", idx,"입니다.")
    return imgs, labels

image_dir_path = "/content/"
(x_train, y_train)=load_data(image_dir_path)
x_train_norm = x_train/255.0   # 입력은 0~1 사이의 값으로 정규화

print("x_train shape: {}".format(x_train.shape))
print("y_train shape: {}".format(y_train.shape))

import matplotlib.pyplot as plt
plt.imshow(x_train[2600])
print('라벨: ', y_train[2600])

x_train.shape

import numpy as np
print('최소값:', np.min(x_train), '최대값:', np.max(x_train))

x_train_norm = x_train / 255.0
x_test_norm = x_test / 255.0
print('최소값:', np.min(x_train_norm), '최대값:', np.max(x_test_norm))

x_train1, x_test, y_train1, y_test = train_test_split(x_train_norm, y_train, test_size=0.2)
x_train, x_valid, y_train, y_valid = train_test_split(x_train1, y_train1, test_size=0.3)

from tensorflow import keras
input_layer = tf.keras.layers.Input(shape=(28,28,3))
x = tf.keras.layers.Dense(900, activation='sigmoid', kernel_initializer=tf.keras.initializers.he_normal())(input_layer)
x = tf.keras.layers.Dense(1800, activation='sigmoid', kernel_initializer=tf.keras.initializers.glorot_uniform)(x)
x = tf.keras.layers.Dense(2700, activation='sigmoid', kernel_initializer=tf.keras.initializers.glorot_uniform)(x)
output_layer = tf.keras.layers.Dense(3, activation='sigmoid')(x)
model = tf.keras.models.Model(inputs=[input_layer], outputs=[output_layer])
model.summary()

model.compile(optimizer='adam',
              loss = 'categorical_crossentropy',
              metrics = ['accuracy'])

history = model.fit(x_train, y_train, epochs=2000, batch_size=100, validation_data=(x_valid, y_valid))

