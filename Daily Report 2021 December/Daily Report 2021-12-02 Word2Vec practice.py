# -*- coding: utf-8 -*-
"""2021-12-02.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rdSOr42fvELtBw9vnS55iJqJSM-nEstR
"""

import pandas as pd
import numpy as np
import urllib.request
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

#위의 WordNetLemmatizer의 역할

# 접사 (affix) ; 단어의 추가적의 의미를 주는 부분
# 어간 (stem) ; 단어의 의미를 담고 있는 단어의 핵심 부분
# cats ; cat(어간) -s(접사)
# 위에서 어간 추출을 함

nltk.download('wordnet')

lemmatizer = WordNetLemmatizer()
words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']
print([lemmatizer.lemmatize(w) for w in words])

lemmatizer.lemmatize('dies','v')

lemmatizer.lemmatize('watched','v')

lemmatizer.lemmatize('has','v')

nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')

urllib.request.urlretrieve("https://raw.githubusercontent.com/franciscadias/data/master/abcnews-date-text.csv",
                           filename="/content/abcnews-data-text.csv")

data = pd.read_csv('/content/abcnews-data-text.csv',error_bad_lines=False)
data

data.head(5)

data.tail(5)

text = data[['headline_text']]

text.nunique()

text.drop_duplicates(inplace=True)
text = text.reset_index(drop=True)
print(len(text))

"""#데이터 정제 및 정규화"""

text['headline_text'] = text.apply(lambda row:nltk.word_tokenize(row['headline_text']),axis=1)

"""### 불용어 제거"""

stop_words = stopwords.words('english')
text['headline_text'] = text['headline_text'].apply(lambda x: [word for word in x if word not in (stop_words)])

text.head()

text['headline_text'] = text['headline_text'].apply(lambda x: [WordNetLemmatizer().lemmatize(word,pos='v') for word in x])

text = text['headline_text'].apply(lambda x: [word for word in x if len(word)>2])

print(text[:5])

#원문으로 복귀
detokenized_doc = []
for i in range(len(text)):
  t = ' '.join(text[i])
  detokenized_doc.append(t)


train_data = detokenized_doc

train_data[:5]

c_vectorizer = CountVectorizer(stop_words='english', max_features = 5000)
document_term_matrix = c_vectorizer.fit_transform(train_data)

print('행렬의 크기 : ', document_term_matrix.shape)

tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features = 5000)
tf_idf_matrix = tfidf_vectorizer.fit_transform(train_data)

print('행렬의 크기 : ',tf_idf_matrix.shape)

data[['publish_date']]

"""### 머신모델로 학습해보기

- KNN
- 나이브베이즈
- 베르누이 나이브 베이즈
- 랜덤포레스트
- SVM
- XGB (boosting기법)
- 결정트리
"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.naive_bayes import BernoulliNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
import xgboost as sgb
from xgboost.sklearn import XGBClassifier

import pandas as pd
import numpy as np
import urllib.request
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

"""dffdfdfdfdfdf"""

#모델 학습 시키기
nbmodel = []

"""# abc 뉴스데이터로 word2vec 실습"""

from nltk.corpus import abc
import nltk
nltk.download('abc')
nltk.download('punkt')

corpus = abc.sents()

print(corpus[:3])

print("코퍼스의 크기", len(corpus))

from gensim.models import Word2Vec

model = Word2Vec(sentences= corpus, size = 100, window=5, min_count=5, workers=4, sg=0) # cbow

model_result = model.wv.most_similar('man')

print(model_result)

from gensim.models import KeyedVectors

model.wv.save_word2vec_format('./w2v')
loaded_model = KeyedVectors.load_word2vec_format('./w2v')
print('모델 load 완료!')

model_result = loaded_model.wv.most_similar('man')

print(model_result)

loaded_model.most_similar('over')

loaded_model.most_similar('memory')

loaded_model.most_similar('overacting')  #비슷한 단어가 없기에 에러가 뜬다

"""# 한국어 word2vec 만들기"""

!pip install konlpy

import pandas as pd
import matplotlib.pyplot as plt
import urllib.request
from gensim.models.word2vec import Word2Vec
from konlpy.tag import Okt

urllib.request.urlretrieve("https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt", filename="ratings.txt")

train_data = pd.read_table('ratings.txt')

train_data[:5]

print(len(train_data))

print(train_data.isnull().values.any()) #널값 존재 유무

train_data = train_data.dropna(how='any') #널 값이 있는 행 제거
print(train_data.isnull().values.any())

print(len(train_data)) #8개 정도 제거

#정규 표현식을 통한 한글 외 문자 제거
train_data['document'] = train_data['document'].str.replace("[^ㄱ-ㅎㅏ-ㅣ가-힣 ]", "")

train_data[:5]

#불용어 제외
stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']

okt = Okt()
tokenized_data = []
for sentence in train_data['document']:
  temp_x = okt.morphs(sentence, stem=True) #토큰화
  temp_x = [word for word in temp_x if not word in stopwords] #불용어 제거
  tokenized_data.append(temp_x)

# 리뷰 길이 분포 확인
print('리뷰의 최대 길이 :', max(len(l) for l in tokenized_data))
print('리뷰의 평균 길이 :', sum(map(len, tokenized_data))/len(tokenized_data))
plt.hist([len(s) for s in tokenized_data], bins=50)
plt.xlabel('length of samples')
plt.ylabel('number of samples')
plt.show()

from gensim.models import Word2Vec
model = Word2Vec(sentences=tokenized_data, size=100, window=5, min_count=5, workers=4, sg=0) # CBOW

model.wv.vectors.shape

print(model.wv.most_similar("최민식"))

print(model.wv.most_similar('히어로'))

"""# 사전 훈련된 워드 임베딩"""

import gensim
model = gensim.models.Word2Vec.load('/content/drive/MyDrive/Colab Notebooks/ko.bin')

result = model.wv.most_similar("강아지")
print(result)

"""#사전 훈련된 워드 임베딩 - 영어"""

import gensim

model = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/MyDrive/Colab Notebooks/GoogleNews-vectors-negative300.bin', binary=True)

print(model.vectors.shape) # 3백만개의 단어와 각단어차원이 300

print(model.similarity('this', 'is'))
print(model.similarity('post','book'))

print(model['book'])

!pip install wikiextractor

# Commented out IPython magic to ensure Python compatibility.
# Colab에 Mecab 설치
!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git
# %cd Mecab-ko-for-Google-Colab
!bash install_mecab-ko_on_colab190912.sh

!wget https://dumps.wikimedia.org/kowiki/latest/kowiki-latest-pages-articles.xml.bz2

!python -m wikiextractor.WikiExtractor kowiki-latest-pages-articles.xml.bz2
# 위키익스트랙터를 사용하여 위키피디아 덤프를 파싱한다.

ls

import os
import re

os.listdir('text')

def list_wiki(dirname):
    filepaths = []
    filenames = os.listdir(dirname)
    for filename in filenames:
        filepath = os.path.join(dirname, filename)

        if os.path.isdir(filepath):
            # 재귀 함수
            filepaths.extend(list_wiki(filepath))
        else:
            find = re.findall(r"wiki_[0-9][0-9]", filepath)
            if 0 < len(find):
                filepaths.append(filepath)
    return sorted(filepaths)

filepaths = list_wiki('text')

len(filepaths)

with open("output_file.txt", "w") as outfile:
    for filename in filepaths:
        with open(filename) as infile:
            contents = infile.read()
            outfile.write(contents)

f = open('output_file.txt', encoding="utf8")

i=0
while True:
    line = f.readline()
    if line != '\n':
        i = i+1
        print("%d번째 줄 :" %i+line)
    if i==10:
        break
f.close()

"""#형태소 분석"""

!pwd

f = open('/content/Mecab-ko-for-Google-Colab/output_file.txt', encoding='utf8')
lines = f.read().splitlines()
print(len(lines))

lines[:10]

from tqdm import tqdm
from konlpy.tag import Mecab

mecab = Mecab()

result = []
for line in tqdm(lines):
  # 빈 문자열이 아닌 경우에만 수행
  if line:
    result.append(mecab.morphs(line))

len(result)

"""## Word2Vec 학습"""

from gensim.models import Word2Vec
model = Word2Vec(result, size=100, window=5, min_count=5, workers=4,sg=0)

model_result1 = model.wv.most_similar("대한민국")
print(model_result1)

model_result2 = model.wv.most_similar("어벤져스")
print(model_result2)

model_result3 = model.wv.most_similar("자주포")
print(model_result3)

