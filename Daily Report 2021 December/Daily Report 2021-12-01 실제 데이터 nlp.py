# -*- coding: utf-8 -*-
"""2021-12-01 실제 데이터 NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GN2ynKrZSzbwebr0dQlcniIzDJdHsBFt
"""

from tensorflow.keras.datasets import reuters
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd

(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=10000, test_split=0.2)

print('훈련 샘플의 수 : {}'.format(len(x_train)))

print('테스트 샘플의 수 : {}'.format(len(x_test)))

print(x_train[0])
print(x_test[0])

print(y_train[0]) # 훈련 기사의 레이블

num_classes = max(y_train)+1
print('클래스의 수 : {}'.format(num_classes))

print('훈련용 뉴스의 최대 길이 : {}'.format(max(len(l) for l in x_train)))
print('훈련용 뉴스의 평균 길이 : {}'.format(sum(map(len, x_train))/len(x_train)))

plt.hist([len(s) for s in x_train], bins= 50)
plt.xlabel('length of samples')
plt.ylabel('number of samples')
plt.show()

fig, axe = plt.subplots(ncols =1)
fig.set_size_inches(12, 5)
sns.countplot(y_train)

unique_elements, counts_elements = np.unique(y_train, return_counts=True)
print("각 클래스 빈도수 : ")
print(np.asarray((unique_elements, counts_elements)))

word_to_index = reuters.get_word_index()
print(word_to_index)

index_to_word = {index +3 : word for word, index in word_to_index.items()}

word_to_index['the']

word_to_index['it']

print(index_to_word[4]) # 빈도수 상위 1번 단어
  print(index_to_word[16]) # 빈도수 상위 13번 단어

# 0 <pad>
# 1 <sos>
# 2 <unk>
for index, token in enumerate(("<pad>", "<sos>", "<unk>")):
    index_to_word[index] = token

print(' '.join([index_to_word[index] for index in x_train[0]]))

# 전체 훈련 데이터에 대해서 decoded
decoded = []
for i in range(len(x_train)):
    t = ' '.join([index_to_word[index] for index in x_train[i]])
    decoded.append(t)

x_train = decoded

# 전체 테스트 데이터에 대해서 decoded
decoded = []
for i in range(len(x_test)):
    t = ' '.join([index_to_word[index] for index in x_test[i]])
    decoded.append(t)

x_test = decoded

x_train[:5]

x_test[:5]

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.naive_bayes import MultinomialNB
from sklearn import metrics

# 단어의 수를 카운트하는 사이킷런의 카운트벡터라이저.
count_vect = CountVectorizer()
# fit_transform : 학습 할 때와 동일한 기반 설정으로 동일하게 테스트 데이터를 변환해야 하는 것
x_train_counts = count_vect.fit_transform(x_train) 

# 카운트벡터라이저의 결과로부터 TF-IDF 결과를 얻습니다.
tfidf_transformer = TfidfTransformer()
x_train_tfidf = tfidf_transformer.fit_transform(x_train_counts)

# 나이브 베이즈 분류기를 수행
# x_train은 TF-IDF의 벡터, y_train 레이블
clf = MultinomialNB().fit(x_train_tfidf, y_train)

def tfidf_vectorizer(data):
    data_counts = count_vect.transform(data)
    data_tfidf = tfidf_transformer.transform(data_counts)
    return data_tfidf

y_pred = clf.predict(tfidf_vectorizer(x_test))

print(metrics.classification_report(y_test, y_pred))

"""LSTM 모델 구현"""

from tensorflow.keras.datasets import reuters
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.naive_bayes import MultinomialNB
from sklearn import metrics

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Embedding
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.models import load_model

import tensorflow as tf
import torch
import torchvision
import torch.nn

# 로이터 로드
(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=1000, test_split=0.2)
# max_len= 100 ---> pad_sequences(입력변수, max_len = 100)
max_len=100
x_train = pad_sequences(x_train, maxlen= max_len)
x_test = pad_sequences(x_test, maxlen=max_len)
# y_train --> one-hot encoding
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

# 모델
# vocab_size = 1000
vocab_size = 1000
# embedding_dim = 128 임베딩이나 히든 유닛 수는 맘대로 해도됨 성능 잘 나오는 것으로
embedding_dim = 128
# hidden_unis = 128
hidden_units = 128
# num_classes = 46
num_classes = 46


model = Sequential()
model.add(Embedding(vocab_size, embedding_dim))
model.add(LSTM(hidden_units))
model.add(Dense(num_classes, activation='softmax'))

#모델 옵션지정
es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience= 4)
mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose = 1, save_best_only=True)

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])

#history = 모델 학습
history = model.fit(x_train, y_train, batch_size = 128, epochs= 30, callbacks=[es, mc], validation_data=(x_test, y_test))

#베스트모델을 h5변수에다가 저장
loaded_model = load_model('/content/best_model.h5')
#테스트 정확도
print('\n 테스트 정확도 : %.4f' % (loaded_model.evaluate(x_test, y_test)[1]))

#train과 test lost function
epochs = range(1, len(history.history['acc'])+1)
plt.plot(epochs, history.history['loss'])
plt.plot(epochs, history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""## 실제 뉴스 기사 크롤링 및 분류"""

!pip install beautifulsoup4
!pip install newspaper3k
!pip install konlpy

!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git

cd Mecab-ko-for-Google-Colab/

!bash install_mecab-ko_on_colab190912.sh

from konlpy.tag import Mecab
mecab = Mecab()
mecab.morphs('하하하하하이')

"""## Beautifulsoup의 사용법"""

from bs4 import BeautifulSoup

html = '''
<html>
    <head>
    </head>
    <body>
        <h1>장바구니
            <p id='clothes' class= 'name' title = '라운드티'> 라운드티
                <span class = 'number'> 25 </span>
                <span class = 'price'> 29000 </span>
                <span class = 'menu'> 의류 </span>
                <a href = "http://www.naver.com"> 바로가기 </a>
            </p>
            <p id='watch' class= 'name' title = '시계'> 시계
                <span class = 'number'> 28 </span>
                <span class = 'price'> 32000 </span>
                <span class = 'menu'> 악세사리 </span>
                <a href = "http://www.facebook.com"> 바로가기 </a>
            </p>
        </h1>
    </body>
</html>
'''

soup = BeautifulSoup(html, 'html.parser')

print(soup.select('body'))

print(soup.select('p'))

print(soup.select('h1 .name .menu'))

print(soup.select('html > h1'))

"""### Newpaper3k 패키지"""

from newspaper import Article

url = 'https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=101&oid=030&aid=0002881076'

article = Article(url, language='ko')
article.download()
article.parse()

print('기사 제목')
print(article.title)

print('기사 내용 :')
print(article.text)

"""### BeautifulSoup와 newspaper3k를 통해 크롤러 만들기"""

import requests
import pandas as pd
from bs4 import BeautifulSoup

def make_urllist(page_num, code, date): 
  urllist= []
  for i in range(1, page_num + 1):
    url = 'https://news.naver.com/main/list.nhn?mode=LSD&mid=sec&sid1='+str(code)+'&date='+str(date)+'&page='+str(i)
    headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.90 Safari/537.36'}
    news = requests.get(url, headers=headers)

    soup = BeautifulSoup(news.content, 'html.parser')

    # CASE 1
    news_list = soup.select('.newsflash_body .type06_headline li dl')
    # CASE 2
    news_list.extend(soup.select('.newsflash_body .type06 li dl'))
        
    # 각 뉴스로부터 a 태그인 <a href ='주소'> 에서 '주소'만을 가져옵니다.
    for line in news_list:
        urllist.append(line.a.get('href'))
  return urllist

url_list =make_urllist(2, 101,20201225)
print('뉴스 기사의 갯수 : ', len(url_list))

url_list[:5]

idx2word = {'101':'경제', '102':'사회','103':'생활/문화','105':'IT/과학'}

from newspaper import Article

#데이터프레임을 생성하는 함수

def make_date(urllist, code):
  text_list = []
  for url in urllist:
    article = Article(url, language='ko')
    article.download()
    article.parse()
    text_list.append(article.text)

  #데이터프레임의 'new'키 아래 파싱한 텍스트를 밸류로 붙여준다
  df = pd.DataFrame({'new':text_list})

  #데이터프레임 'code'키 아래 한글 카테고리명을 붙여준다
  df['code'] = idx2word[str(code)]
  return df

data = make_date(url_list, 101)
data[:10]

"""#데이터 수집 및 전처리"""

code_list = [102,103,105]
code_list

def make_total_date(page_num,code_list, date):

  df = None

  for code in code_list:
    url_list = make_urllist(page_num, code, date)
    df_temp = make_date(url_list,code)
    print(str(code)+ '번 코드에 대한 데이터를 만들었습니다.')

    if df is not None:
      df = pd.concat([df, df_temp])
    else:
      df= df_temp

  return df

df=make_total_date(1, code_list, 20201225)

print('뉴스 기사의 갯수 : ',len(df))

df.sample(10)

df=make_total_date(100, code_list, 20201225)

import os 

csv_path = './news_data.csv'

df.to_csv(csv_path, index=False)

if os.path.exists(csv_path):
  print('{} File Saved1'.format(csv_path))

df = pd.read_table(csv_path, sep=',')
df.head()

df['new'] = df['new'].str.replace("[^ㄱ-ㅎㅏ-ㅣ가-힣] ", "")
df['new']

print(df.isnull().sum())

df.drop_duplicates(subset=['new'], inplace=True)
print('뉴스 기사의 갯수 :', len(df))

"""### 데이터 탐색"""

df['code'].value_counts().plot(kind='bar')

print(df.groupby('code').size().reset_index(name='count'))

from konlpy.tag import Mecab
tokenizer = Mecab()

kor_text = '밤에 귀가하던 여성에게 범죄를 시도한 대 남성이 구속됐다서울 제주경찰서는 \
            상해 혐의로 씨를 구속해 수사하고 있다고 일 밝혔다씨는 지난달 일 피해 여성을 \
            인근 지하철 역에서부터 따라가 폭행을 시도하려다가 도망간 혐의를 받는다피해 \
            여성이 저항하자 놀란 씨는 도망갔으며 신고를 받고 주변을 수색하던 경찰에 \
            체포됐다피해 여성은 이 과정에서 경미한 부상을 입은 것으로 전해졌다'

print(tokenizer.morphs(kor_text))

"""### 불용어 제거"""

stopwords = ['에','는','은','을','했','에게','있','이','의','하','한','다','과','때문','할','수','무단','따른','및','금지','전재','경향신문','기자','는데','가','등','들','파이낸셜','저작','등','뉴스']

def preprocessing(data):
  text_data = []

  for sentence in data:
    temp_data = []
    temp_data = tokenizer.morphs(sentence)
    temp_data = [word for word in temp_data if not word in stopwords]
    text_data.append(temp_data)

  text_data = list(map(' '.join,text_data))
  return text_data

text_data = preprocessing(df['new'])

print(text_data[0])

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.naive_bayes import MultinomialNB
from sklearn import metrics

x_train, x_test, y_train, y_test = train_test_split(text_data, df['code'], random_state=0)

print('훈련용 뉴스 기사의 갯수 : ', len(x_train))
print('테스트 뉴스 기사의 갯수 : ', len(x_test))
print('훈련용 레이블의 갯수 : ', len(y_train))
print('테스트 레이블의 갯수 : ', len(y_test))

#DTM
count_vect = CountVectorizer()
x_train_couts = count_vect.fit_transform(x_train)

tfidf_transformer = TfidfTransformer()
x_train_tfidf = tfidf_transformer.fit_transform(x_train_couts)

#모델 학습 (나이브베이즈 모델)
clf = MultinomialNB().fit(x_train_tfidf, y_train)

"""### 평가 및 테스트"""

def tfidf_vectorizer(data):
  data_counts = count_vect.transform(data)
  data_tfidf = tfidf_transformer.transform(data_counts)
  return data_tfidf

new_sent = preprocessing(["민주당 일각에서 법사위의 체계·자구 심사 기능을 없애야 한다는 \
                           주장이 나오는데 대해 “체계·자구 심사가 법안 지연의 수단으로 \
                          쓰이는 것은 바람직하지 않다”면서도 “국회를 통과하는 법안 중 위헌\
                          법률이 1년에 10건 넘게 나온다. 그런데 체계·자구 심사까지 없애면 매우 위험하다”고 반박했다."                        
])

print(clf.predict(tfidf_vectorizer(new_sent)))

new_sent2 = preprocessing(["인도 로맨틱 코미디 영화 <까립까립 싱글>(2017)을 봤을 때 나는 두 눈을 의심했다. \
                          저 사람이 남자 주인공이라고? 노안에 가까운 이목구비와 기름때로 뭉친 파마머리와, \
                          대충 툭툭 던지는 말투 등 전혀 로맨틱하지 않은 외모였다. 반감이 일면서 \
                          ‘난 외모지상주의자가 아니다’라고 자부했던 나에 대해 회의가 들었다.\
                           티브이를 꺼버릴까? 다른 걸 볼까? 그런데, 이상하다. 왜 이렇게 매력 있지? 개구리와\
                            같이 툭 불거진 눈망울 안에는 어떤 인도 배우에게서도 느끼지 못한 \
                            부드러움과 선량함, 무엇보다 슬픔이 있었다. 2시간 뒤 영화가 끝나고 나는 완전히 이 배우에게 빠졌다"])

print(clf.predict(tfidf_vectorizer(new_sent2)))

y_pred = clf.predict(tfidf_vectorizer(x_test))
print(metrics.classification_report(y_test, y_pred))